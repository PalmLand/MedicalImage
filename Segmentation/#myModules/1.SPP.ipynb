{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 空间金字塔池化(Spatial Pyramid Pooling, SPP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: https://arxiv.org/pdf/1406.4729.pdf\n",
    "\n",
    "![SPP](https://img-blog.csdnimg.cn/img_convert/a6f8c813f7aee5efc91d20f80bef17db.png)\n",
    "\n",
    "$H_{out}=floor(\\frac{H_{in}+2padding[0]-kernerl\\_size[0]}{stride[0]}+1)$\n",
    "\n",
    "$W_{out}=floor(\\frac{W_{in}+2padding[1]-kernerl\\_size[1]}{stride[1]}+1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPP out shape = torch.Size([2, 2048, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SPP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SPP,self).__init__()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=5,stride=1,padding=5//2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=7,stride=1,padding=7//2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=13,stride=1,padding=13//2)\n",
    "        \n",
    "    def forward(self,x):    \n",
    "        x1 = self.pool1(x)\n",
    "        x2 = self.pool2(x)\n",
    "        x3 = self.pool3(x)\n",
    "        return torch.cat([x,x1,x2,x3],dim=1)\n",
    "    \n",
    "# 输入的一批数据的数目2\n",
    "# 输入的通道数512\n",
    "# 高度和宽度都是13\n",
    "x = torch.rand((2,512,13,13))    \n",
    "f = SPP()\n",
    "#print(f(x))\n",
    "print('SPP out shape =',f(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPP能在输入尺寸任意的情况下产生固定大小的输出，而以前的深度网络中的滑窗池化(sliding window pooling)则不能\n",
    "> SPP就是为了解决CNN输入图像大小必须固定的问题，从而可以使得输入图像高宽比和大小任意。\n",
    "\n",
    "* $n$是池化后矩阵的大小，$K_h$和$K_w$分别是kernel的高和宽，$S_h$和$S_w$分别是stride的高和宽，$p_h$和$p_w$分别是padding的高和宽，$h_new$和$w_new$分别是新的高和宽。\n",
    "\n",
    "* 核和步长的计算公式都使用的是**ceil()，即向上取整**，而padding使用的是**floor()，即向下取整**。\n",
    "\n",
    "$K_h =  ⌈\\frac{h_{in}}{n}⌉=ceil(\\frac{h_{in}}{n})$\n",
    "\n",
    "$S_h = ⌈\\frac{h_{in}}{n}⌉=ceil(\\frac{h_{in}}{n})$\n",
    "\n",
    "$p_h = ⌊\\frac{k_h*n-h_{in}+1}{2}⌋=floor(\\frac{k_h*n-h_{in}+1}{2})$\n",
    "\n",
    "$h_{new} = 2*p_h +h_{in}$\n",
    "\n",
    "....\n",
    "\n",
    "$K_w =  ⌈\\frac{w_{in}}{n}⌉=ceil(\\frac{w_{in}}{n})$\n",
    "\n",
    "$S_w = ⌈\\frac{w_{in}}{n}⌉=ceil(\\frac{w_{in}}{n})$\n",
    "\n",
    "$p_w = ⌊\\frac{k_w*n-w_{in}+1}{2}⌋=floor(\\frac{k_w*n-w_{in}+1}{2})$\n",
    "\n",
    "$w_{new} = 2*p_w +w_{in}$\n",
    "\n",
    "假设输入数据大小和上面一样是(10,7,11), 池化数量为(4,4)：\n",
    "\n",
    "Kernel大小为(2,3)，Stride大小为(2,3)，所以Padding为(1,1)。\n",
    "\n",
    "利用矩阵大小计算公式：$⌊\\frac{h+2p-f}{s}+1⌋*⌊\\frac{w+2p-f}{s}+1⌋$得到池化后的矩阵大小为：$4∗4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "import torch,math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 构建SPP层(空间金字塔池化层)\n",
    "class SPPLayer(torch.nn.Module):\n",
    "    def __init__(self, num_levels, pool_type='max_pool'):\n",
    "        super(SPPLayer, self).__init__()\n",
    "        self.num_levels = num_levels\n",
    "        self.pool_type = pool_type\n",
    "        \n",
    "    def forward(self,x):\n",
    "        num,c,h,w = x.size()# num:样本数量 c:通道数 h:高 w:宽\n",
    "        for i in range(self.num_levels):\n",
    "            level = i+1\n",
    "            kernel_size = (math.ceil(h / level), math.ceil(w / level))\n",
    "            stride = (math.ceil(h / level), math.ceil(w / level))\n",
    "            pooling = (math.floor((kernel_size[0]*level-h+1)/2), math.floor((kernel_size[1]*level-w+1)/2))\n",
    "                      \n",
    "            # 选择池化方式 \n",
    "            if self.pool_type == 'max_pool':\n",
    "                tensor = F.max_pool2d(x, kernel_size=kernel_size, stride=stride, padding=pooling).view(num, -1)\n",
    "            else:\n",
    "                tensor = F.avg_pool2d(x, kernel_size=kernel_size, stride=stride, padding=pooling).view(num, -1)\n",
    "                \n",
    "            print('level =',level)\n",
    "            print('kernel_size =',kernel_size)\n",
    "            print('stride =',stride)\n",
    "            print('pooling =',pooling) \n",
    "            print(F.max_pool2d(x, kernel_size=kernel_size, stride=stride, padding=pooling).shape)\n",
    "            print('*'*30)\n",
    "            \n",
    "            # 展开、拼接\n",
    "            if (i == 0):\n",
    "                x_flatten = tensor.view(num, -1)\n",
    "            # 第一个之后都要拼接上去\n",
    "            else:\n",
    "                x_flatten = torch.cat((x_flatten, tensor.view(num, -1)), 1)\n",
    "        return x_flatten        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level = 1\n",
      "kernel_size = (15, 15)\n",
      "stride = (15, 15)\n",
      "pooling = (0, 0)\n",
      "torch.Size([2, 512, 1, 1])\n",
      "******************************\n",
      "level = 2\n",
      "kernel_size = (8, 8)\n",
      "stride = (8, 8)\n",
      "pooling = (1, 1)\n",
      "torch.Size([2, 512, 2, 2])\n",
      "******************************\n",
      "SPP out shape = torch.Size([2, 2560])\n",
      "\n",
      "2560 = 512*1*1+512*2*2\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2,512,15,15))    \n",
    "f = SPPLayer(2)\n",
    "print('SPP out shape =',f(x).shape)\n",
    "print('\\n2560 = 512*1*1+512*2*2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
