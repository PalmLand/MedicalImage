{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从`torchvision`载入resnet18预训练模型。\n",
    "\n",
    "创建一个随机tensor来表示一个带有3个通道、64*64的图像，随机初始化其对应的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data  = torch.rand(1,3,64,64)\n",
    "labels = torch.rand(1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来在模型的每一层运行输入数据以做出预测，这是向前传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算loss，然后通过网络反向传播这个loss。\n",
    "\n",
    "当对这个loss 张量调用`backward()`时，就会启动反向传播。Autograd利用参数的`.grad`属性中计算并存储每个模型参数的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入优化器，learning rate为0.01,momentum为0.9。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.01\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim = torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9)\n",
    "optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后调用`.step()`启动梯度下降，优化器通过存储在`.grad`中的梯度来调整每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()#gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd的工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用`requires_grad=True`创建两个张量a和b，这个标记表明a和b的每个操作都应该被跟踪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([2.,3.],requires_grad=True)\n",
    "b = torch.tensor([6.,4.],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000012ABDB18188>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2)\n",
    "print(x.requires_grad)\n",
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)\n",
    "y = (x * x).sum()\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设a和b是NN的参数，Q是误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q grad_fn3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当我们在Q上调用`.backward()`时，autograd计算这些梯度并将它们存储在各自张量的`.grad`属性中。**\n",
    "\n",
    "在`Q.backward()`中需要显式传递一个`gradient`参数，因为它是一个向量。`gradient`是一个与Q形状相同的张量，它表示Q自身的梯度，\n",
    "\n",
    "我们还可以将Q聚合为一个标量并隐式向后调用，如`Q.sum().backward()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "# 36 = 9*2*2*1\n",
    "# 810 = 2*3*3*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)\n",
    "# -12 = -2*6*1\n",
    "# -80 = -2*4*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# 检查得到的gradient是否正确\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过将代码包裹在`with torch.no_grad()`，来停止对从跟踪历史中 的`.requires_grad=True`的张量自动求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "操作的输出张量将需要梯度，即使只有一个输入张量是`requires_grad=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,5)\n",
    "y = torch.rand(5,5)\n",
    "z = torch.rand((5,5),requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在预训练的微调中，冻结大部分模型，通常只修改分类器层以对新标签做出预测。\n",
    "\n",
    "我们加载一个预先训练好的`resnet18`模型，并冻结所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,optim\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于resnet，分类器是最后一层的`model.fc`。\n",
    "\n",
    "我们可以简单地用一个新的线性层(默认未冻结)代替它，作为我们的分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在模型中的所有参数，除了`model.fc`的参数。计算梯度的唯一参数是`model.fc`的权值和偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.fc.parameters(),lr=1e-2,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "唯一计算梯度(并因此在梯度下降中更新)的参数是分类器的权值和偏差。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
